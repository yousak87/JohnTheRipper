/*
 * Copyright (c) 2011 Samuele Giovanni Tonon
 * samu at linuxasylum dot net
 * Copyright (c) 2012, magnum
 * and Copyright (c) 2013, Sayantan Datta <std2048 at gmail.com>
 * This program comes with ABSOLUTELY NO WARRANTY; express or
 * implied .
 * This is free software, and you are welcome to redistribute it
 * under certain conditions; as expressed here
 * http://www.gnu.org/licenses/gpl-2.0.html
 */

#include <string.h>
#include <math.h>

#include "path.h"
#include "arch.h"
#include "misc.h"
#include "common.h"
#include "formats.h"
#include "sha.h"
#include "johnswap.h"
#include "common-opencl.h"
#include "options.h"
#include "loader.h"
#include "opencl_rawsha1_fmt.h"

#define FORMAT_LABEL			"Raw-SHA1-opencl"
#define FORMAT_NAME			""
#define ALGORITHM_NAME			"SHA1 OpenCL (inefficient, development use only)"

#define BENCHMARK_COMMENT		""
#define BENCHMARK_LENGTH		-1

#define PLAINTEXT_LENGTH    		55 /* Max. is 55 with current kernel */
#define BUFSIZE				((PLAINTEXT_LENGTH+3)/4*4)
#define HASH_LENGTH			(2 * DIGEST_SIZE)
#define CIPHERTEXT_LENGTH		(HASH_LENGTH + TAG_LENGTH)

#define DIGEST_SIZE			20
#define BINARY_SIZE			20
#define BINARY_ALIGN			4
#define SALT_SIZE			0
#define SALT_ALIGN			1

#define MIN_KEYS_PER_CRYPT		(1024*2048*8)
#define MAX_KEYS_PER_CRYPT		MIN_KEYS_PER_CRYPT

#define FORMAT_TAG			"$dynamic_26$"
#define TAG_LENGTH			(sizeof(FORMAT_TAG) - 1)

#define CONFIG_NAME			"rawsha1"

#ifndef uint32_t
#define uint32_t unsigned int
#endif

#define RAWSHA1_DEBUG 0

typedef struct {
	uint32_t h0,h1,h2,h3,h4;
} SHA_DEV_CTX;

//cl_command_queue queue_prof;

cl_mem pinned_saved_keys, pinned_saved_idx, pinned_partial_hashes, buffer_out;
cl_mem buffer_keys, buffer_idx;
static cl_uint *partial_hashes, *res_hashes;
static unsigned int *saved_plain, num_keys = 0;
static uint64_t *saved_idx, key_idx = 0;
static int have_full_hashes;

cl_mem buffer_ld_hashes, buffer_bitmap1, buffer_bitmap2, buffer_outKeyIdx;
static unsigned int *loaded_hashes, *outKeyIdx, cmp_out = 0;
static struct bitmap_context_mixed *bitmap1;
static struct bitmap_context_global *bitmap2;
static int loaded_count = 0;

cl_mem buffer_mask_gpu;
static struct mask_context msk_ctx;
static struct db_main *DB;
static unsigned char *mask_offsets;
static unsigned int mask_mode = 0;

cl_kernel crk_kernel, crk_kernel_mm, crk_kernel_om;

static unsigned int self_test = 1; // Used as a flag

#define MIN(a, b)		(((a) > (b)) ? (b) : (a))
#define MAX(a, b)		(((a) > (b)) ? (a) : (b))

static struct fmt_tests tests[] = {
	{"a9993e364706816aba3e25717850c26c9cd0d89d", "abc"},
	{FORMAT_TAG "095bec1163897ac86e393fa16d6ae2c2fce21602", "7850"},
	{"dd3fbb0ba9e133c4fd84ed31ac2e5bc597d61774", "7858"},
	{NULL}
};

static void set_key(char *_key, int index);
static int crypt_all(int *pcount, struct db_salt *_salt);
static int crypt_all_self_test(int *pcount, struct db_salt *_salt);
static char *get_key_self_test(int index);
static char *get_key(int index);

static int valid(char *ciphertext, struct fmt_main *self){
	int i;

	if (!strncmp(ciphertext, FORMAT_TAG, TAG_LENGTH))
		ciphertext += TAG_LENGTH;

	if (strlen(ciphertext) != HASH_LENGTH) return 0;
	for (i = 0; i < HASH_LENGTH; i++){
		if (!((('0' <= ciphertext[i]) && (ciphertext[i] <= '9')) ||
			(('a' <= ciphertext[i]) && (ciphertext[i] <= 'f'))
			|| (('A' <= ciphertext[i]) && (ciphertext[i] <= 'F'))))
			return 0;
	}
	return 1;
}

static char *split(char *ciphertext, int index, struct fmt_main *self)
{
	static char out[CIPHERTEXT_LENGTH + 1];

	if (!strncmp(ciphertext, FORMAT_TAG, TAG_LENGTH))
		ciphertext += TAG_LENGTH;

	strncpy(out, FORMAT_TAG, sizeof(out));

	memcpy(&out[TAG_LENGTH], ciphertext, HASH_LENGTH);
	out[CIPHERTEXT_LENGTH] = 0;

	strlwr(&out[TAG_LENGTH]);

	return out;
}

static void create_clobj(int kpc){
	pinned_saved_keys = clCreateBuffer(context[ocl_gpu_id], CL_MEM_READ_ONLY | CL_MEM_ALLOC_HOST_PTR, BUFSIZE*kpc, NULL, &ret_code);
	HANDLE_CLERROR(ret_code, "Error creating page-locked memory");
	saved_plain = clEnqueueMapBuffer(queue[ocl_gpu_id], pinned_saved_keys, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE, 0, BUFSIZE*kpc, 0, NULL, NULL, &ret_code);
	HANDLE_CLERROR(ret_code, "Error mapping page-locked memory saved_plain");

	pinned_saved_idx = clCreateBuffer(context[ocl_gpu_id], CL_MEM_READ_ONLY | CL_MEM_ALLOC_HOST_PTR, sizeof(uint64_t) * kpc, NULL, &ret_code);
	HANDLE_CLERROR(ret_code, "Error creating page-locked memory pinned_saved_idx");
	saved_idx = clEnqueueMapBuffer(queue[ocl_gpu_id], pinned_saved_idx, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE, 0, sizeof(uint64_t) * kpc, 0, NULL, NULL, &ret_code);
	HANDLE_CLERROR(ret_code, "Error mapping page-locked memory saved_idx");

	res_hashes = malloc(sizeof(cl_uint) * 4 * kpc);

	pinned_partial_hashes = clCreateBuffer(context[ocl_gpu_id], CL_MEM_WRITE_ONLY | CL_MEM_ALLOC_HOST_PTR, sizeof(cl_uint) * kpc, NULL, &ret_code);
	HANDLE_CLERROR(ret_code, "Error creating page-locked memory");
	partial_hashes = (cl_uint *) clEnqueueMapBuffer(queue[ocl_gpu_id], pinned_partial_hashes, CL_TRUE, CL_MAP_READ, 0, sizeof(cl_uint) * kpc, 0, NULL, NULL, &ret_code);
	HANDLE_CLERROR(ret_code, "Error mapping page-locked memory partial_hashes");

	buffer_keys = clCreateBuffer(context[ocl_gpu_id], CL_MEM_READ_ONLY, BUFSIZE * kpc, NULL, &ret_code);
	HANDLE_CLERROR(ret_code, "Error creating buffer keys argument");
	buffer_idx = clCreateBuffer(context[ocl_gpu_id], CL_MEM_READ_ONLY, sizeof(uint64_t) * kpc, NULL, &ret_code);
	HANDLE_CLERROR(ret_code, "Error creating buffer argument buffer_idx");

	buffer_out = clCreateBuffer(context[ocl_gpu_id], CL_MEM_WRITE_ONLY, sizeof(cl_uint) * 5 * kpc, NULL, &ret_code);
	HANDLE_CLERROR(ret_code, "Error creating buffer out argument");

	HANDLE_CLERROR(clSetKernelArg(crypt_kernel, 0, sizeof(buffer_keys), (void *) &buffer_keys), "Error setting argument 0");
	HANDLE_CLERROR(clSetKernelArg(crypt_kernel, 1, sizeof(buffer_idx), (void *) &buffer_idx), "Error setting argument 1");
	HANDLE_CLERROR(clSetKernelArg(crypt_kernel, 2, sizeof(buffer_out), (void *) &buffer_out), "Error setting argument 2");

	global_work_size = kpc;
}

static void release_clobj(void){
	HANDLE_CLERROR(clEnqueueUnmapMemObject(queue[ocl_gpu_id], pinned_partial_hashes, partial_hashes, 0,NULL,NULL), "Error Unmapping partial_hashes");
	HANDLE_CLERROR(clEnqueueUnmapMemObject(queue[ocl_gpu_id], pinned_saved_keys, saved_plain, 0, NULL, NULL), "Error Unmapping saved_plain");
	HANDLE_CLERROR(clEnqueueUnmapMemObject(queue[ocl_gpu_id], pinned_saved_idx, saved_idx, 0, NULL, NULL), "Error Unmapping saved_idx");

	HANDLE_CLERROR(clReleaseMemObject(buffer_keys), "Error Releasing buffer_keys");
	HANDLE_CLERROR(clReleaseMemObject(buffer_idx), "Error Releasing buffer_idx");
	HANDLE_CLERROR(clReleaseMemObject(buffer_out), "Error Releasing buffer_out");
	HANDLE_CLERROR(clReleaseMemObject(pinned_saved_keys), "Error Releasing pinned_saved_keys");
	HANDLE_CLERROR(clReleaseMemObject(pinned_partial_hashes), "Error Releasing pinned_partial_hashes");
	MEM_FREE(res_hashes);

	if(!self_test) {

		MEM_FREE(loaded_hashes);
		MEM_FREE(outKeyIdx);
		MEM_FREE(mask_offsets);
		MEM_FREE(bitmap1);
		MEM_FREE(bitmap2);

		HANDLE_CLERROR(clReleaseMemObject(buffer_ld_hashes), "Release loaded hashes");
		HANDLE_CLERROR(clReleaseMemObject(buffer_outKeyIdx), "Release output key indices");
		HANDLE_CLERROR(clReleaseMemObject(buffer_bitmap1), "Release bitmap buffer");
		HANDLE_CLERROR(clReleaseMemObject(buffer_bitmap2), "Release bitmap buffer");
		HANDLE_CLERROR(clReleaseMemObject(buffer_mask_gpu), "Release gpu mask buffer");
	}
}

static void done(void)
{
	release_clobj();

	HANDLE_CLERROR(clReleaseKernel(crypt_kernel), "Release kernel");
	HANDLE_CLERROR(clReleaseKernel(crk_kernel_mm), "Release kernel");
	HANDLE_CLERROR(clReleaseKernel(crk_kernel_om), "Release kernel");
	HANDLE_CLERROR(clReleaseProgram(program[ocl_gpu_id]), "Release Program");
}

/*
   this function could be used to calculated the best num
   of keys per crypt for the given format
*/
/*
static void find_best_kpc(void){
	int num;
	cl_event myEvent;
	cl_ulong startTime, endTime, tmpTime;
	int kernelExecTimeNs = INT_MAX;
	cl_int ret_code;
	int optimal_kpc=2048;
	int i = 0;
	cl_uint *tmpbuffer;

	fprintf(stderr, "Calculating best keys per crypt, this will take a while ");
	for( num=MAX_KEYS_PER_CRYPT; num >= 4096 ; num -= 4096){
		release_clobj();
		create_clobj(num);
		advance_cursor();
		queue_prof = clCreateCommandQueue( context[ocl_gpu_id], devices[ocl_gpu_id], CL_QUEUE_PROFILING_ENABLE, &ret_code);
		for (i=0; i < num; i++)
			set_key(tests[0].plaintext, i);

		clEnqueueWriteBuffer(queue[ocl_gpu_id], buffer_keys, CL_TRUE, 0, 4 * key_idx, saved_plain, 0, NULL, NULL), "failed in clEnqueueWriteBuffer buffer_keys";
		clEnqueueWriteBuffer(queue[ocl_gpu_id], buffer_idx, CL_TRUE, 0, 4 * global_work_size, saved_idx, 0, NULL, NULL), "failed in clEnqueueWriteBuffer buffer_idx";

		ret_code = clEnqueueNDRangeKernel( queue_prof, crypt_kernel, 1, NULL, &global_work_size, &local_work_size, 0, NULL, &myEvent);
		if(ret_code != CL_SUCCESS){
			fprintf(stderr, "Error %d\n",ret_code);
			continue;
		}
		clFinish(queue_prof);
		clGetEventProfilingInfo(myEvent, CL_PROFILING_COMMAND_SUBMIT, sizeof(cl_ulong), &startTime, NULL);
		clGetEventProfilingInfo(myEvent, CL_PROFILING_COMMAND_END  , sizeof(cl_ulong), &endTime  , NULL);
		tmpTime = endTime-startTime;
		tmpbuffer = mem_alloc(sizeof(cl_uint) * num);
		clEnqueueReadBuffer(queue_prof, buffer_out, CL_TRUE, 0, sizeof(cl_uint) * num, tmpbuffer, 0, NULL, &myEvent);
		clGetEventProfilingInfo(myEvent, CL_PROFILING_COMMAND_SUBMIT, sizeof(cl_ulong), &startTime, NULL);
		clGetEventProfilingInfo(myEvent, CL_PROFILING_COMMAND_END  , sizeof(cl_ulong), &endTime  , NULL);
		tmpTime = tmpTime + (endTime-startTime);
		if( ((int)( ((float) (tmpTime) / num) * 10 )) <= kernelExecTimeNs) {
			kernelExecTimeNs = ((int) (((float) (tmpTime) / num) * 10) ) ;
			optimal_kpc = num;
		}
		MEM_FREE(tmpbuffer);
		clReleaseCommandQueue(queue_prof);
	}
	fprintf(stderr, "Optimal keys per crypt %d\n(to avoid this test on next run do export GWS=%d)\n",optimal_kpc,optimal_kpc);
	global_work_size = optimal_kpc;
	release_clobj();
	create_clobj(optimal_kpc);
}*/

static void fmt_rawsha1_init(struct fmt_main *self) {
	//char *temp;
	//cl_ulong maxsize;

	local_work_size = global_work_size = 0;

	opencl_init("$JOHN/kernels/sha1_kernel.cl", ocl_gpu_id, NULL);

	// create kernel to execute
	crypt_kernel = clCreateKernel(program[ocl_gpu_id], "sha1_self_test", &ret_code);
	HANDLE_CLERROR(ret_code, "Error creating kernel. Double-check kernel name?");
	crk_kernel_mm = clCreateKernel(program[ocl_gpu_id], "sha1_mm", &ret_code);
	HANDLE_CLERROR(ret_code, "Error creating kernel. Double-check kernel name?");
	crk_kernel_om = clCreateKernel(program[ocl_gpu_id], "sha1_om", &ret_code);
	HANDLE_CLERROR(ret_code, "Error creating kernel. Double-check kernel name?");

	/* Note: we ask for the kernels' max sizes, not the device's! */
	//HANDLE_CLERROR(clGetKernelWorkGroupInfo(crypt_kernel, devices[ocl_gpu_id], CL_KERNEL_WORK_GROUP_SIZE, sizeof(maxsize), &maxsize, NULL), "Query max workgroup size");
	local_work_size = global_work_size = 0;
	opencl_get_user_preferences(CONFIG_NAME);

	/* Round off to nearest power of 2 */
	if(local_work_size)
		local_work_size = pow(2, ceil(log(local_work_size)/log(2)));
	if(!global_work_size)
		global_work_size = MAX_KEYS_PER_CRYPT;
	if(!local_work_size)
		local_work_size = LWS;

	if (options.mask) {
		local_work_size = LWS;
		global_work_size /= 4;
		mask_mode = 1;
	}

	create_clobj((global_work_size + local_work_size - 1) / local_work_size * local_work_size);

	if (options.verbosity > 2)
		fprintf(stderr, "Local worksize (LWS) %d, Global worksize (GWS) %d\n",(int)local_work_size, (int)global_work_size);

	self->params.max_keys_per_crypt = global_work_size;
	self->params.min_keys_per_crypt = local_work_size;
	self->methods.crypt_all = crypt_all_self_test;
	self->methods.get_key = get_key_self_test;

}

static void clear_keys(void)
{
	key_idx = 0;
	num_keys = 0;
}

static void set_kernel_args(cl_kernel *kernel) {
	int argIndex = 0;
	HANDLE_CLERROR(clSetKernelArg(*kernel, argIndex++, sizeof(buffer_keys), (void*) &buffer_keys),
		"Error setting argument 0");
	HANDLE_CLERROR(clSetKernelArg(*kernel, argIndex++, sizeof(buffer_idx), (void*) &buffer_idx ),
		"Error setting argument 1");
	HANDLE_CLERROR(clSetKernelArg(*kernel, argIndex++, sizeof(buffer_ld_hashes), (void*) &buffer_ld_hashes ),
		"Error setting argument 2");
	HANDLE_CLERROR(clSetKernelArg(*kernel, argIndex++, sizeof(buffer_outKeyIdx), (void*) &buffer_outKeyIdx ),
		"Error setting argument 3");
	HANDLE_CLERROR(clSetKernelArg(*kernel, argIndex++, sizeof(buffer_bitmap1), (void*) &buffer_bitmap1),
		"Error setting argument 4");
	HANDLE_CLERROR(clSetKernelArg(*kernel, argIndex++, sizeof(buffer_bitmap2), (void*) &buffer_bitmap2),
		"Error setting argument 5");
	if(mask_mode)
		HANDLE_CLERROR(clSetKernelArg(*kernel, argIndex++, sizeof(buffer_mask_gpu), (void*) &buffer_mask_gpu),
			"Error setting argument 6");
}

static void opencl_sha1_reset(struct db_main *db) {

	if(db) {
		int length = 0;

		db->format->params.min_keys_per_crypt = local_work_size;

		loaded_hashes = (unsigned int*)mem_alloc(((db->password_count) * 5 + 1)*sizeof(unsigned int));
		outKeyIdx     = (unsigned int*)mem_calloc((db->password_count) * sizeof(unsigned int) * 2);
		mask_offsets  = (unsigned char*) mem_calloc(db->format->params.max_keys_per_crypt);
		bitmap1       = (struct bitmap_context_mixed*)mem_alloc(sizeof(struct bitmap_context_mixed));
		bitmap2       = (struct bitmap_context_global*)mem_alloc(sizeof(struct bitmap_context_global));

		buffer_ld_hashes = clCreateBuffer(context[ocl_gpu_id], CL_MEM_READ_WRITE, ((db->password_count) * 5 + 1)*sizeof(int), NULL, &ret_code);
		HANDLE_CLERROR(ret_code, "Error creating buffer arg loaded_hashes\n");
		length = ((db->format->params.max_keys_per_crypt) > ((db->password_count) * sizeof(unsigned int) * 2)) ?
			  (db->format->params.max_keys_per_crypt) : ((db->password_count) * sizeof(unsigned int) * 2);
		/* buffer_outKeyIdx is multiplexed for use as mask_offset input and keyIdx output */
		buffer_outKeyIdx = clCreateBuffer(context[ocl_gpu_id], CL_MEM_READ_WRITE, length, NULL, &ret_code);
		HANDLE_CLERROR(ret_code, "Error creating buffer outkeyIdx\n");
		buffer_bitmap1 = clCreateBuffer(context[ocl_gpu_id], CL_MEM_READ_WRITE, sizeof(struct bitmap_context_mixed), NULL, &ret_code);
		HANDLE_CLERROR(ret_code, "Error creating buffer arg loaded_hashes\n");
		buffer_bitmap2 = clCreateBuffer(context[ocl_gpu_id], CL_MEM_READ_WRITE, sizeof(struct bitmap_context_global), NULL, &ret_code);
		HANDLE_CLERROR(ret_code, "Error creating buffer arg loaded_hashes\n");
		buffer_mask_gpu = clCreateBuffer(context[ocl_gpu_id], CL_MEM_READ_ONLY, sizeof(struct mask_context) , NULL, &ret_code);
		HANDLE_CLERROR(ret_code, "Error creating buffer mask gpu\n");

		self_test = 0;

		if (mask_mode) {

			set_kernel_args(&crk_kernel_mm);
			crk_kernel = crk_kernel_mm;
			db -> max_int_keys = 26 * 26 * 10;
			DB = db;
		}

		else {
			set_kernel_args(&crk_kernel_om);
			crk_kernel = crk_kernel_om;
		}

		db->format->methods.crypt_all = crypt_all;
		db->format->methods.get_key = get_key;
	}
}

static void load_hash(struct db_salt *salt) {

	unsigned int *bin, i;
	struct db_password *pw;

	loaded_count = (salt->count);
	loaded_hashes[0] = loaded_count;
	pw = salt -> list;
	i = 0;
	do {
		bin = (unsigned int *)pw -> binary;
		// Potential segfault if removed
		if(bin != NULL) {
			loaded_hashes[i + 1] = bin[0];
			loaded_hashes[i + 1 + loaded_count] = bin[1];
			loaded_hashes[i + 1 + 2 * loaded_count] = bin[2];
			loaded_hashes[i + 1 + 3 * loaded_count] = bin[3];
			loaded_hashes[i + 1 + 4 * loaded_count] = bin[4];
			i++ ;
		}
	} while ((pw = pw -> next)) ;

	if(i != (salt->count)) {
		fprintf(stderr, "Something went wrong while loading hashes to gpu..Exiting..\n");
		exit(EXIT_FAILURE);
	}

	HANDLE_CLERROR(clEnqueueWriteBuffer(queue[ocl_gpu_id], buffer_ld_hashes, CL_TRUE, 0, (i * 5 + 1) * sizeof(unsigned int) , loaded_hashes, 0, NULL, NULL), "failed in clEnqueueWriteBuffer loaded_hashes");
}

static void load_bitmap(unsigned int num_loaded_hashes, unsigned int index, unsigned int *bitmap, size_t szBmp) {
	unsigned int i, hash;
	memset(bitmap, 0, szBmp);

	for(i = 0; i < num_loaded_hashes; i++) {
		hash = loaded_hashes[index * num_loaded_hashes + i + 1] & (szBmp * 8 - 1);
		// divide by 32 , harcoded here and correct only for unsigned int
		bitmap[hash >> 5] |= (1U << (hash & 31));
	}
}

static void load_hashtable_plus(unsigned int *hashtable, unsigned int *loaded_next_hash, unsigned int idx, unsigned int num_loaded_hashes, unsigned int szHashTbl) {
	unsigned int i;
#if RAWSHA1_DEBUG
	unsigned int counter = 0;
#endif
	memset(hashtable, 0xFF, szHashTbl * sizeof(unsigned int));
	memset(loaded_next_hash, 0xFF, num_loaded_hashes * sizeof(unsigned int));

	for (i = 0; i < num_loaded_hashes; ++i) {
		unsigned int hash = loaded_hashes[i + idx*num_loaded_hashes + 1] & (szHashTbl - 1);
		loaded_next_hash[i] = hashtable[hash];
#if RAWSHA1_DEBUG
		if(!(hashtable[hash]^0xFFFFFFFF)) counter++;
#endif
		hashtable[hash] = i;
	}
#if RAWSHA1_DEBUG
	fprintf(stderr, "Hash Table Effectiveness:%lf%%\n", ((double)counter/(double)num_loaded_hashes)*100);
#endif
}

static void check_mask_sha1(struct mask_context *msk_ctx) {
	int i, j, k ;

	if(msk_ctx -> count > PLAINTEXT_LENGTH) msk_ctx -> count = PLAINTEXT_LENGTH;
	if(msk_ctx -> count > MASK_RANGES_MAX) {
		fprintf(stderr, "MASK parameters are too small...Exiting...\n");
		exit(EXIT_FAILURE);

	}

  /* Assumes msk_ctx -> activeRangePos[] is sorted. Check if any range exceeds nt key limit */
	for( i = 0; i < msk_ctx->count; i++)
		if(msk_ctx -> activeRangePos[i] >= PLAINTEXT_LENGTH) {
			msk_ctx->count = i;
			break;
		}
	j = 0;
	i = 0;
	k = 0;
 /* Append non-active portion to activeRangePos[] for ease of computation inside GPU */
	while((j <= msk_ctx -> activeRangePos[k]) && (k < msk_ctx -> count)) {
		if(j == msk_ctx -> activeRangePos[k]) {
			k++;
			j++;
			continue;
		}
		msk_ctx -> activeRangePos[msk_ctx -> count + i] = j;
		i++;
		j++;
	}
	while ((i+msk_ctx->count) < MASK_RANGES_MAX) {
		msk_ctx -> activeRangePos[msk_ctx -> count + i] = j;
		i++;
		j++;
	}

	for(i = msk_ctx->count; i < MASK_RANGES_MAX; i++)
		msk_ctx->ranges[msk_ctx -> activeRangePos[i]].count = 0;

	/* Sort active ranges in descending order of charchter count */
	if(msk_ctx->ranges[msk_ctx -> activeRangePos[0]].count < msk_ctx->ranges[msk_ctx -> activeRangePos[1]].count) {
		i = msk_ctx -> activeRangePos[1];
		msk_ctx -> activeRangePos[1] = msk_ctx -> activeRangePos[0];
		msk_ctx -> activeRangePos[0] = i;
	}

	if(msk_ctx->ranges[msk_ctx -> activeRangePos[0]].count < msk_ctx->ranges[msk_ctx -> activeRangePos[2]].count) {
		i = msk_ctx -> activeRangePos[2];
		msk_ctx -> activeRangePos[2] = msk_ctx -> activeRangePos[0];
		msk_ctx -> activeRangePos[0] = i;
	}

	if(msk_ctx->ranges[msk_ctx -> activeRangePos[1]].count < msk_ctx->ranges[msk_ctx -> activeRangePos[2]].count) {
		i = msk_ctx -> activeRangePos[2];
		msk_ctx -> activeRangePos[2] = msk_ctx -> activeRangePos[1];
		msk_ctx -> activeRangePos[1] = i;
	}

	/* Consecutive charchters in ranges that have all the charchters consective are
	 * arranged in ascending order. This is to make password generation on host and device
	 * match each other for md5_ccc and md5_cnn kernels*/
	for( i = 0; i < msk_ctx->count; i++)
		if(msk_ctx->ranges[msk_ctx -> activeRangePos[i]].start != 0) {
			for (j = 0; j < msk_ctx->ranges[msk_ctx -> activeRangePos[i]].count; j++)
				msk_ctx->ranges[msk_ctx -> activeRangePos[i]].chars[j] =
					msk_ctx->ranges[msk_ctx -> activeRangePos[i]].start + j;

		}
}

static void load_mask(struct db_main *db) {
	if (!db->msk_ctx) {
		fprintf(stderr, "No given mask.Exiting...\n");
		exit(EXIT_FAILURE);
	}
	memcpy(&msk_ctx, db->msk_ctx, sizeof(struct mask_context));
	check_mask_sha1(&msk_ctx);
#if RAWSHA1_DEBUG
	int i, j;
	for(i = 0; i < MASK_RANGES_MAX; i++)
	    printf("%d ",msk_ctx.activeRangePos[i]);
	printf("\n");
	for(i = 0; i < MASK_RANGES_MAX; i++)
	    printf("%d ",msk_ctx.ranges[msk_ctx.activeRangePos[i]].count);
	printf("\n");

	/*
	for(i = 0; i < msk_ctx.count; i++)
	  printf(" %d ", msk_ctx.activeRangePos[i]);*/
	for(i = 0; i < msk_ctx.count; i++){
			for(j = 0; j < msk_ctx.ranges[msk_ctx.activeRangePos[i]].count; j++)
				printf("%c ",msk_ctx.ranges[msk_ctx.activeRangePos[i]].chars[j]);
			printf("\n");
			//checkRange(&msk_ctx, msk_ctx.activeRangePos[i]) ;
			printf("START:%c",msk_ctx.ranges[msk_ctx.activeRangePos[i]].start);
			printf("\n");
	}
#endif
	HANDLE_CLERROR(clEnqueueWriteBuffer(queue[ocl_gpu_id], buffer_mask_gpu, CL_TRUE, 0, sizeof(struct mask_context), &msk_ctx, 0, NULL, NULL ), "Failed Copy data to gpu");
}


static void set_key(char *_key, int index)
{
	const ARCH_WORD_32 *key = (ARCH_WORD_32*)_key;
	int len = strlen(_key);

	saved_idx[index] = (key_idx << 6) | len;

	while (len > 4) {
		saved_plain[key_idx++] = *key++;
		len -= 4;
	}
	if (len)
		saved_plain[key_idx++] = *key & (0xffffffffU >> (32 - (len << 3)));

	num_keys++;
}

static char *get_key_self_test(int index)
{
	static char out[PLAINTEXT_LENGTH + 1];
	int i, len = saved_idx[index] & 63;
	char *key = (char*)&saved_plain[saved_idx[index] >> 6];

	for (i = 0; i < len; i++)
		out[i] = key[i];
	out[i] = 0;
	return out;
}

static void passgen(int ctr, int offset, char *key) {
	int i, j, k;

	offset = msk_ctx.flg_wrd ? offset : 0;

	i =  ctr % msk_ctx.ranges[msk_ctx.activeRangePos[0]].count;
	key[msk_ctx.activeRangePos[0] + offset] = msk_ctx.ranges[msk_ctx.activeRangePos[0]].chars[i];

	if (msk_ctx.ranges[msk_ctx.activeRangePos[1]].count) {
		j = (ctr / msk_ctx.ranges[msk_ctx.activeRangePos[0]].count) % msk_ctx.ranges[msk_ctx.activeRangePos[1]].count;
		key[msk_ctx.activeRangePos[1] + offset] = msk_ctx.ranges[msk_ctx.activeRangePos[1]].chars[j];
	}
	if (msk_ctx.ranges[msk_ctx.activeRangePos[2]].count) {
		k = (ctr / (msk_ctx.ranges[msk_ctx.activeRangePos[0]].count * msk_ctx.ranges[msk_ctx.activeRangePos[1]].count)) % msk_ctx.ranges[msk_ctx.activeRangePos[2]].count;
		key[msk_ctx.activeRangePos[2] + offset] = msk_ctx.ranges[msk_ctx.activeRangePos[2]].chars[k];
	}
}

static char *get_key(int index)
{
	static char out[PLAINTEXT_LENGTH + 1];
	int i , len;
	char *key;
	int ctr = 0, mask_offset = 0, flag = 0;

	if((index < loaded_count) && cmp_out) {
		ctr = outKeyIdx[index + loaded_count];
		/* outKeyIdx contains all zero when no new passwords are cracked.
		 * Hence during status checks even if index is less than loaded count
		 * correct range of passwords is displayed.
		 */
		index = outKeyIdx[index] & 0x7fffffff;
		mask_offset = mask_offsets[index];
		flag = 1;
	}
	index = (index > num_keys)? (num_keys?num_keys-1:0): index;
	len = saved_idx[index] & 63;
	key = (char*)&saved_plain[saved_idx[index] >> 6];

	for (i = 0; i < len; i++)
		out[i] = key[i];

	if(cmp_out && mask_mode && flag)
		passgen(ctr, mask_offset, out);

	out[i] = 0;
	return out;
}

static void *binary(char *ciphertext)
{
	static unsigned char *realcipher;
	int i;

	if (!realcipher)
		realcipher = mem_alloc_tiny(DIGEST_SIZE, MEM_ALIGN_WORD);

	ciphertext += TAG_LENGTH;

	for(i=0;i<DIGEST_SIZE;i++)
	{
		realcipher[i] = atoi16[ARCH_INDEX(ciphertext[i*2])]*16 +
			atoi16[ARCH_INDEX(ciphertext[i*2+1])];
	}
	return (void *) realcipher;
}

static int cmp_all(void *binary, int count)
{
	unsigned int i;
	unsigned int b = ((unsigned int *) binary)[0];

	if(!self_test) return 1;

	for (i = 0; i < count; i++)
		if (b == partial_hashes[i])
			return 1;
	return 0;
}

static int cmp_one(void *binary, int index)
{
	return (((unsigned int*)binary)[0] == partial_hashes[index]);
}

static int cmp_exact(char *source, int count) {

	if(self_test || cmp_out) {
		unsigned int *t = (unsigned int *) binary(source);
		unsigned int num = self_test ? global_work_size: loaded_count;
		if(self_test) {
			if (!have_full_hashes){
				clEnqueueReadBuffer(queue[ocl_gpu_id], buffer_out, CL_TRUE,
					sizeof(cl_uint) * num,
					sizeof(cl_uint) * 4 * num, res_hashes, 0,
					NULL, NULL);
				have_full_hashes = 1;
			}

			if (t[1]!=res_hashes[count])
				return 0;
			if (t[2]!=res_hashes[1 * num + count])
				return 0;
			if (t[3]!=res_hashes[2 * num + count])
				return 0;
			if (t[4]!=res_hashes[3 * num + count])
				return 0;
			return 1;
		}
		else {
			if(!outKeyIdx[count]) return 0;
			if (t[1]!=loaded_hashes[count + num + 1])
				return 0;
			if (t[2]!=loaded_hashes[2 * num + count +1])
				return 0;
			if (t[3]!=loaded_hashes[3 * num  + count + 1])
				return 0;
			return 1;
		}
	}

	return 0;
}

static int crypt_all_self_test(int *pcount, struct db_salt *salt)
{
	int count = *pcount;

	global_work_size = (count + local_work_size - 1) / local_work_size * local_work_size;

	HANDLE_CLERROR(clEnqueueWriteBuffer(queue[ocl_gpu_id], buffer_keys, CL_TRUE, 0, 4 * key_idx, saved_plain, 0, NULL, NULL), "failed in clEnqueueWriteBuffer buffer_keys");
	HANDLE_CLERROR(clEnqueueWriteBuffer(queue[ocl_gpu_id], buffer_idx, CL_TRUE, 0, sizeof(uint64_t) * global_work_size, saved_idx, 0, NULL, NULL), "failed in clEnqueueWriteBuffer buffer_idx");

	HANDLE_CLERROR(clEnqueueNDRangeKernel(queue[ocl_gpu_id], crypt_kernel, 1, NULL, &global_work_size, &local_work_size, 0, NULL, profilingEvent), "failed in clEnqueueNDRangeKernel");

	HANDLE_CLERROR(clFinish(queue[ocl_gpu_id]),"failed in clFinish");

	// read back partial hashes
	HANDLE_CLERROR(clEnqueueReadBuffer(queue[ocl_gpu_id], buffer_out, CL_TRUE, 0, sizeof(cl_uint) * global_work_size, partial_hashes, 0, NULL, NULL), "failed in reading data back");
	have_full_hashes = 0;

	return count;
}


static int crypt_all(int *pcount, struct db_salt *salt)
{
	int count = *pcount, i;
	static unsigned int flag, multiplier;

	global_work_size = (count + local_work_size - 1) / local_work_size * local_work_size;

	if(!flag && mask_mode) {
		load_mask(DB);
		multiplier = 1;
		for (i = 0; i < msk_ctx.count; i++)
			multiplier *= msk_ctx.ranges[msk_ctx.activeRangePos[i]].count;
//#if RAWSHA1_DEBUG
		fprintf(stderr, "c/s rate shown in status report is buggy. Multiply the p/s rate with:%d to get actual c/s rate.\n", multiplier);
//#endif
		flag = 1;
	}

	if(mask_mode)
		*pcount *= multiplier;

	if(loaded_count != (salt->count)) {
		load_hash(salt);
		load_bitmap(loaded_count, 0, &bitmap1[0].bitmap0[0], (BITMAP_SIZE_1 / 8));
		load_bitmap(loaded_count, 1, &bitmap1[0].bitmap1[0], (BITMAP_SIZE_1 / 8));
		load_bitmap(loaded_count, 2, &bitmap1[0].bitmap2[0], (BITMAP_SIZE_1 / 8));
		load_bitmap(loaded_count, 3, &bitmap1[0].bitmap3[0], (BITMAP_SIZE_1 / 8));
		load_bitmap(loaded_count, 0, &bitmap1[0].gbitmap0[0], (BITMAP_SIZE_3 / 8));
		load_hashtable_plus(&bitmap2[0].hashtable0[0], &bitmap1[0].loaded_next_hash[0], 2, loaded_count, HASH_TABLE_SIZE_0);
		HANDLE_CLERROR(clEnqueueWriteBuffer(queue[ocl_gpu_id], buffer_bitmap1, CL_TRUE, 0, sizeof(struct bitmap_context_mixed), bitmap1, 0, NULL, NULL ), "Failed Copy data to gpu");
		HANDLE_CLERROR(clEnqueueWriteBuffer(queue[ocl_gpu_id], buffer_bitmap2, CL_TRUE, 0, sizeof(struct bitmap_context_global), bitmap2, 0, NULL, NULL ), "Failed Copy data to gpu");
	}

	HANDLE_CLERROR(clEnqueueWriteBuffer(queue[ocl_gpu_id], buffer_keys, CL_TRUE, 0, 4 * key_idx, saved_plain, 0, NULL, NULL), "failed in clEnqueueWriteBuffer buffer_keys");
	HANDLE_CLERROR(clEnqueueWriteBuffer(queue[ocl_gpu_id], buffer_idx, CL_TRUE, 0, sizeof(uint64_t) * global_work_size, saved_idx, 0, NULL, NULL), "failed in clEnqueueWriteBuffer buffer_idx");

	if(msk_ctx.flg_wrd)
		HANDLE_CLERROR(clEnqueueWriteBuffer(queue[ocl_gpu_id], buffer_outKeyIdx, CL_TRUE, 0,
			(DB->format->params.max_keys_per_crypt), mask_offset_buffer, 0, NULL, NULL),
			"failed in clEnqueWriteBuffer buffer_outKeyIdx");

	HANDLE_CLERROR(clEnqueueNDRangeKernel(queue[ocl_gpu_id], crk_kernel, 1, NULL, &global_work_size, &local_work_size, 0, NULL, profilingEvent), "failed in clEnqueueNDRangeKernel");

	HANDLE_CLERROR(clFinish(queue[ocl_gpu_id]),"failed in clFinish");

	// read back compare results
	HANDLE_CLERROR(clEnqueueReadBuffer(queue[ocl_gpu_id], buffer_outKeyIdx, CL_TRUE, 0, sizeof(cl_uint) * loaded_count, outKeyIdx, 0, NULL, NULL), "failed in reading cracked key indices back");
	cmp_out = 0;

	// If a positive match is found outKeyIdx contains some positive value else contains 0
	for(i = 0; i < (loaded_count & (~cmp_out)); i++)
		cmp_out = outKeyIdx[i]?0xffffffff:0;

	if(cmp_out) {
		HANDLE_CLERROR(clEnqueueReadBuffer(queue[ocl_gpu_id], buffer_outKeyIdx, CL_TRUE, 0, sizeof(cl_uint) * loaded_count * 2, outKeyIdx, 0, NULL, NULL), "failed in reading cracked key indices back");
		for(i = 0; i < loaded_count; i++) {
			if(outKeyIdx[i])
				partial_hashes[i] = loaded_hashes[i+1];
			else partial_hashes[i] = 0;
		}
		if(msk_ctx.flg_wrd)
			memcpy(mask_offsets, mask_offset_buffer, (DB->format->params.max_keys_per_crypt));
		have_full_hashes = 0;
		return loaded_count;
	}

	else return 0;
}

static int get_hash_0(int index) { return partial_hashes[index] & 0xf; }
static int get_hash_1(int index) { return partial_hashes[index] & 0xff; }
static int get_hash_2(int index) { return partial_hashes[index] & 0xfff; }
static int get_hash_3(int index) { return partial_hashes[index] & 0xffff; }
static int get_hash_4(int index) { return partial_hashes[index] & 0xfffff; }
static int get_hash_5(int index) { return partial_hashes[index] & 0xffffff; }
static int get_hash_6(int index) { return partial_hashes[index] & 0x7ffffff; }

struct fmt_main fmt_opencl_rawSHA1 = {
	{
		FORMAT_LABEL,
		FORMAT_NAME,
		ALGORITHM_NAME,
		BENCHMARK_COMMENT,
		BENCHMARK_LENGTH,
		PLAINTEXT_LENGTH,
		BINARY_SIZE,
		BINARY_ALIGN,
		SALT_SIZE,
		SALT_ALIGN,
		MIN_KEYS_PER_CRYPT,
		MAX_KEYS_PER_CRYPT,
		FMT_CASE | FMT_8_BIT | FMT_SPLIT_UNIFIES_CASE,
		tests
	}, {
		fmt_rawsha1_init,
		done,
		opencl_sha1_reset,
		fmt_default_prepare,
		valid,
		split,
		binary,
		fmt_default_salt,
		fmt_default_source,
		{
			fmt_default_binary_hash_0,
			fmt_default_binary_hash_1,
			fmt_default_binary_hash_2,
			fmt_default_binary_hash_3,
			fmt_default_binary_hash_4,
			fmt_default_binary_hash_5,
			fmt_default_binary_hash_6
		},
		fmt_default_salt_hash,
		fmt_default_set_salt,
		set_key,
		get_key,
		clear_keys,
		crypt_all_self_test,
		{
			get_hash_0,
			get_hash_1,
			get_hash_2,
			get_hash_3,
			get_hash_4,
			get_hash_5,
			get_hash_6
		},
		cmp_all,
		cmp_one,
		cmp_exact
	}

};
